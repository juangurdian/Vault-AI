version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: beast-ollama
    ports:
      - "11434:11434"
    volumes:
      - ${USERPROFILE}/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/11434' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  backend:
    build:
      context: ..
      dockerfile: backend/Dockerfile
    container_name: beast-backend
    ports:
      - "8001:8001"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - SEARXNG_BASE_URL=http://searxng:8080
      - API_PREFIX=/api
      - LOG_LEVEL=INFO
    depends_on:
      ollama:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8001/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  frontend:
    build:
      context: ..
      dockerfile: frontend/Dockerfile
      args:
        - NEXT_PUBLIC_API_BASE=http://localhost:8001/api
    container_name: beast-frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_BASE=http://localhost:8001/api
    depends_on:
      backend:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  comfyui:
    profiles: ["heavy"]
    image: comfyanonymous/comfyui:latest
    container_name: beast-comfyui
    ports:
      - "8188:8188"
    volumes:
      - comfyui_models:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/system_stats"]
      interval: 10s
      timeout: 5s
      retries: 3

  searxng:
    profiles: ["heavy"]
    image: searxng/searxng:latest
    container_name: beast-searxng
    ports:
      - "8080:8080"
    volumes:
      - ./searxng:/etc/searxng
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 3

volumes:
  ollama_models:
    driver: local
  comfyui_models:
    driver: local

networks:
  default:
    name: beast-network

