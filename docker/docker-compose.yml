version: "3.9"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: beast-ollama
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0

  backend:
    build:
      context: ../
      dockerfile: backend/Dockerfile
    container_name: beast-backend
    restart: unless-stopped
    depends_on:
      - ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - HOST=0.0.0.0
      - PORT=8001
    ports:
      - "${BACKEND_PORT:-8001}:8001"

  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    container_name: beast-frontend
    restart: unless-stopped
    depends_on:
      - backend
    environment:
      - NEXT_PUBLIC_API_BASE=http://localhost:${BACKEND_PORT:-8001}/api
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    profiles: ["ui"]

  comfyui:
    image: ghcr.io/comfyanonymous/comfyui:latest
    container_name: beast-comfyui
    restart: unless-stopped
    ports:
      - "${COMFYUI_PORT:-8188}:8188"
    volumes:
      - comfyui_models:/root/ComfyUI/models/checkpoints
    profiles: ["heavy"]

  searxng:
    image: searxng/searxng:latest
    container_name: beast-searxng
    restart: unless-stopped
    ports:
      - "${SEARXNG_PORT:-8080}:8080"
    environment:
      - SEARXNG_BASE_URL=http://localhost:${SEARXNG_PORT:-8080}
    profiles: ["heavy"]

volumes:
  ollama_models:
  comfyui_models:

